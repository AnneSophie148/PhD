<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A novel dataset and deep learning object detection benchmark for grapevine pest surveillance</title>
				<funder ref="#_jBG8Gbr">
					<orgName type="full">European Union Next-GenerationEU</orgName>
				</funder>
				<funder>
					<orgName type="full">Interconnected Nord-Est Innovation Ecosystem (iNEST)</orgName>
				</funder>
				<funder ref="#_KdkfdCn">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-12">12 December 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giorgio</forename><surname>Checola</surname></persName>
							<email>giorgio.checola@fmach.it</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Innovation Centre</orgName>
								<address>
									<addrLine>Fondazione Edmund Mach San Michele all&apos;Adige</addrLine>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paolo</forename><surname>Sonego</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Innovation Centre</orgName>
								<address>
									<addrLine>Fondazione Edmund Mach San Michele all&apos;Adige</addrLine>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Zorer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Innovation Centre</orgName>
								<address>
									<addrLine>Fondazione Edmund Mach San Michele all&apos;Adige</addrLine>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Valerio</forename><surname>Mazzoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research and Innovation Centre</orgName>
								<address>
									<addrLine>Fondazione Edmund Mach San Michele all&apos;Adige</addrLine>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franca</forename><surname>Ghidoni</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Technology Transfer Centre</orgName>
								<orgName type="institution">University of Southern Queensland</orgName>
								<address>
									<addrLine>Fondazione Edmund Mach San Michele all&apos;Adige</addrLine>
									<region>TN</region>
									<country>Italy Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Gelmetti</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Technology Transfer Centre</orgName>
								<orgName type="institution">University of Southern Queensland</orgName>
								<address>
									<addrLine>Fondazione Edmund Mach San Michele all&apos;Adige</addrLine>
									<region>TN</region>
									<country>Italy Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Franceschi</surname></persName>
							<email>pietro.franceschi@fmach.it</email>
							<affiliation key="aff0">
								<orgName type="department">Research and Innovation Centre</orgName>
								<address>
									<addrLine>Fondazione Edmund Mach San Michele all&apos;Adige</addrLine>
									<region>TN</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jenny</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nagaraju</forename><surname>Yalavarthi</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Central Silk Board</orgName>
								<orgName type="institution">Jian Lian</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shandong Management University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A novel dataset and deep learning object detection benchmark for grapevine pest surveillance</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-12">12 December 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">2DCF3738270D063CA5A90D210F42D7C0</idno>
					<idno type="DOI">10.3389/fpls.2024.1485216</idno>
					<note type="submission">RECEIVED 23 August 2024 ACCEPTED 20 November 2024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-05-13T22:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=true, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scaphoideus titanus</term>
					<term>insect detection</term>
					<term>yellow sticky traps</term>
					<term>deep learning</term>
					<term>machine vision</term>
					<term>precision agriculture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s>Flavescence doreé (FD) poses a significant threat to grapevine health, with the American grapevine leafhopper, Scaphoideus titanus, serving as the primary vector.</s><s>FD is responsible for yield losses and high production costs due to mandatory insecticide treatments, infected plant uprooting, and replanting.</s><s>Another potential FD vector is the mosaic leafhopper, Orientus ishidae, commonly found in agroecosystems.</s><s>The current monitoring approach, which involves periodic human identification of yellow sticky traps, is labor-intensive and time-consuming.</s><s>Therefore, there is a compelling need to develop an automatic pest detection system leveraging recent advances in computer vision and deep learning techniques.</s><s>However, progress in developing such a system has been hindered by the lack of effective datasets for training.</s><s>To fill this gap, our study contributes a fully annotated dataset of S. titanus and O. ishidae from yellow sticky traps, which includes more than 600 images, with approximately 1500 identifications per class.</s><s>Assisted by entomologists, we performed the annotation process, trained, and compared the performance of two state-ofthe-art object detection algorithms: YOLOv8 and Faster R-CNN.</s><s>Pre-processing, including automatic cropping to eliminate irrelevant background information and image enhancements to improve the overall quality of the dataset, was employed.</s><s>Additionally, we tested the impact of altering image resolution and data augmentation, while also addressing potential issues related to class detection.</s><s>The results, evaluated through 10-fold cross validation, revealed promising detection accuracy, with YOLOv8 achieving an mAP@0.5 of 92%, and an F1score above 90%, with an mAP@[0.5:0.95] of 66%.</s><s>Meanwhile, Faster R-CNN reached an mAP@0.5 and mAP@[0.5:0.95] of 86% and 55%, respectively.</s><s>This outcome offers encouraging prospects for developing more effective management strategies in the fight against Flavescence doreé.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p><s>Among grapevine adversities, Flavescence doreé (FD) is the most severe phytoplasma disease in Europe and for this reason is subject to quarantine measures across the European Union <ref type="bibr">(EFSA et al., 2020)</ref>.</s><s>In response to the worsening impact and damages caused by this harmful phytoplasma, the Italian Ministry of Agriculture, Food Sovereignty, and Forests redefined emergency phytosanitary measures in 2023, issuing the Order No. 22/06/2023, No. 4 (G.U.</s><s>12/08/2023, n. 188).</s><s>First discovered in Italy during the early 1970s, it has since spread to most viticultural regions, with epidemic episodes peaking at the end of the century <ref type="bibr" target="#b31">(Morone et al., 2007)</ref>.</s><s>FD infection results from the interaction between a phytoplasma and an insect vector, primarily Scaphoideus titanus (ST), the American grapevine leafhopper, which is monophagous on Vitis plants <ref type="bibr" target="#b29">(Lessio et al., 2014)</ref>.</s><s>Nymphs appear in May, while adults emerge at the beginning of July.</s><s>Both nymphs and adults can acquire the phytoplasma while feeding on infected plants.</s><s>Once infected, they remain carriers for the rest of their lives, transmitting the pathogen from one grapevine to another <ref type="bibr" target="#b18">(Gonella et al., 2024)</ref>.</s><s>Due to its small size, ranging from 4.8 to 5.8 mm, identifying ST without adequate magnification is challenging even for entomologists.</s><s>Another emerging vector is Orientus ishidae (OI), also known as the mosaic leafhopper due to the characteristic color pattern of its wings (Figure <ref type="figure" target="#fig_0">1</ref>) <ref type="bibr" target="#b14">(Gaffuri et al., 2011;</ref><ref type="bibr" target="#b27">Lessio et al., 2019)</ref>.</s><s>Despite the lower transmission efficiency compared to S. titanus <ref type="bibr" target="#b28">(Lessio et al., 2016)</ref>, its widespread presence also in other agroecosystems, such as apple orchards <ref type="bibr" target="#b9">(Dalmaso et al., 2023)</ref>, make it a potential concern.</s></p><p><s>The current management strategy for controlling FD involves limiting the spread of the vector through the timely application of insecticides, primarily targeting juveniles, and uprooting the affected plants to prevent the disease from spreading.</s><s>However, since it has been discovered that adults can also acquire and transmit FD very efficiently, monitoring the dynamics of the ST population has become fundamentally important to determine whether a summer insecticide treatment is necessary <ref type="bibr">(Alma et al., 2018, p. 201)</ref>.</s><s>Surveillance of S. titanus and O. ishidae adults rely on sticky card traps <ref type="bibr" target="#b35">(Pavan et al., 2021)</ref>, which are left in the vineyard for 7-15 days and then manually checked for the presence of vectors by expert operators.</s><s>Following trap collection, insect identification is performed in the laboratory by using a stereoscopic microscope.</s><s>This approach, albeit reliable, is time consuming and represents a bottleneck towards the development of large scale real-time monitoring.</s></p><p><s>In recent years, technology advancements in FD management have involved two main research avenues <ref type="bibr" target="#b26">(Lee and Tardaguila, 2023)</ref>:</s></p><p><s>1.</s><s>The automated monitoring of vector spread using machine vision techniques on insect traps: such solutions could significantly enhance disease control by enabling realtime mapping and generating large datasets of digitized trap images, allowing for retrospective investigations into the spread of other potential vectors.</s><s>Two researchers <ref type="bibr" target="#b11">(Ding and Taylor, 2016)</ref> pioneered the use of convolutional neural networks (CNNs) for detecting moths from trap images.</s><s>Subsequently, several studies have applied similar algorithms to yellow sticky traps for various purposes, including monitoring vine pests <ref type="bibr" target="#b2">(Bessa, 2021;</ref><ref type="bibr" target="#b16">Goncalves et al., 2022)</ref>, detecting other species of insects, such as Scirtothrips dorsalis <ref type="bibr" target="#b32">(Niyigena et al., 2023)</ref> and the European cherry fruit fly <ref type="bibr" target="#b37">(Salamut et al., 2023)</ref>.</s><s>Moreover, significant research efforts have been dedicated to developing easily deployable trap systems for real-time detection <ref type="bibr" target="#b5">(Bjerge et al., 2021</ref><ref type="bibr" target="#b4">(Bjerge et al., , 2022;;</ref><ref type="bibr" target="#b25">Le et al., 2021;</ref><ref type="bibr" target="#b41">Suto, 2022;</ref><ref type="bibr" target="#b3">Bjerge et al., 2023;</ref><ref type="bibr" target="#b39">Sittinger et al., 2023)</ref>.</s><s>Some manufacturers have made available new smart traps for the detection of ST adults, e.g., Trapview (<ref type="url" target="https://trapview.com/">https://trapview.com/</ref>) and iSCOUT ® COLOR TRAP by Metos (<ref type="url" target="https://metos.global/en/iscout/">https://metos.global/en/iscout/</ref>).</s><s>Finally, other studies have leveraged open datasets, such as iNaturalist, to benchmark state-of-the-art models for multi-species detection <ref type="bibr" target="#b0">(Ahmad et al., 2022;</ref><ref type="bibr" target="#b24">Kumar et al., 2023;</ref><ref type="bibr" target="#b45">Wang et al., 2023)</ref>, but these solutions were not specifically developed for research grade applications.</s><s>2. Vineyard monitoring using imaging techniques on symptomatic plants: computer vision combined with multispectral and hyperspectral imaging or remote sensing has shown promising results for early detection of grapevine diseases <ref type="bibr" target="#b38">(Silva et al., 2022;</ref><ref type="bibr" target="#b43">Tardif et al., 2022</ref><ref type="bibr" target="#b42">Tardif et al., , 2023))</ref>.</s></p><p><s>The focus of this study concerns the first line of research, specifically FD vectors captured by sticky card traps.</s><s>In this area, the lack of high-quality datasets necessary for model training represents the major limitation to the development of automatic monitoring solutions of ST.</s><s>In this paper, we address this gap by presenting and making available a fully annotated dataset of yellow sticky trap images, with insect identifications carried out by a team of expert entomologists.</s><s>The dataset has to be intended as a reference source for establishing an autonomous and accurate pest identification system against the FD spread.</s><s>In addition, we demonstrate its potential use by benchmarking two state-of-the-art object detection architectures with different image processing techniques.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data collection</head><p><s>The efficiency of a deep learning model depends on the quality and quantity of data used for the training.</s><s>Considering the scope of our investigation, we focused on yellow sticky traps (YST) (Glutor, Biogard ® , 10x25 cm), positioned in vineyards from different sites in Trentino (northern Italy) from July to November 2023, when ST adults occur.</s><s>YST were exposed for a maximum of 14 days.</s></p><p><s>Due to the practical challenges in sample collection, images were obtained through four distinct methods (Table <ref type="table" target="#tab_0">1</ref>):</s></p><p><s>• photos taken directly in the field;</s></p><p><s>• images of stored YST (T = 5 ± 1°C), and deceased reared i n s e c t s o n e m p t y t r a p s w i t h i n a c o n t r o l l e d greenhouse environment; • digital scans of YST collected during regular monitoring activities in the fields; • photos from a smart trap prototype installed in our experimental vineyard (Figure <ref type="figure" target="#fig_1">2</ref>).</s></p><p><s>Digitally scanned YST made the primary contribution to our dataset since this method of acquisition allowed us to avoid common camera issues related to external interference, such as focusing and lighting, while also maximizing resolution.</s><s>However, it presented drawbacks such as suboptimal visual conditions due to the risk of insect squeezing during the scanning process and possible reflections of nylon bags in which yellow traps are stored.</s></p><p><s>Regarding the smart trap, the device consists of several commercial components mounted on a customized printed circuit board (PCB).</s><s>Specifically, the diurnal 3-hourly (9:00 AM, 12:00 AM, 3:00 PM, 6:00 PM) time-lapse images have been captured by the 8 MPixel Raspberry Pi camera module V2 (Raspberry Pi Foundation, Cambridge, UK) connected to the Raspberry Pi zero W single board computer.</s><s>Each image consists of 3280 × 2462 pixels and the final size of the jpg file is about 5 MBytes.</s><s>A Witty Pi 3 mini clock and power management board controlled the ON/OFF scheduled sequence.</s><s>Images were sent back to the server via WiFi, by means of a Secure Copy Protocol (SCP) file transfer protocol.</s><s>These images were repurposed as background images given the absence of target insects due to mandatory treatments against the spread of FD.</s></p><p><s>The final dataset consists of 615 images which also include the images of 150 traps where the two target insects were not detected.</s><s>These were included to add variety so that the network can properly learn to distinguish the target objects from other insects.</s><s>Insect annotations comprise 1329 ST and 1506 for OI, ensuring an almost class-balanced dataset.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data pre-processing</head><p><s>An automated cropping procedure, inspired by <ref type="bibr" target="#b2">(Bessa, 2021)</ref>, was implemented using the Python library OpenCV <ref type="bibr">(Bradski, 2000)</ref> to remove unnecessary background information outside of the yellow trap.</s><s>The workflow is outlined in Figure <ref type="figure">3</ref>.</s></p><p><s>The original images were first converted to the HSV (Hue, Saturation, Value) color space and then segmented by defining two yellow thresholds.</s><s>Subsequently, the algorithm identified contours in the binary mask image and extracted the largest contour based on its area.</s><s>Using the coordinates of the bounding rectangle around this contour, the cropping operation on the original images was performed.</s></p><p><s>Before proceeding with data annotation, enhancement techniques were applied to the images to improve image quality and consequently model performance <ref type="bibr" target="#b11">(Ding and Taylor, 2016</ref>; Flowchart diagram of data pre-processing operations.</s><s>Smart-trap prototype (A) and its camera framing (B).</s></p><p><s>Checola et al. 10.3389/fpls.2024.1485216</s><s><ref type="bibr" target="#b34">Pang et al., 2022;</ref><ref type="bibr" target="#b41">Suto, 2022)</ref>.</s><s>Specifically, brightness, contrast and sharpness parameters were adjusted to increase insect visibility and reduce the impact of lighting variations.</s><s>Using OpenCV, the addWeighted function modifies brightness and contrast by calculating the weighted sum of two arrays as: a Á image + b Á image + g .</s><s>We set a = 1:1, b = 10 and g = 0 to meet visual requirements.</s><s>Additionally, a sharpening filter was applied using the filter2D function to enhance image details.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Object detection models</head><p><s>Object detection tasks perform both localization and class recognition, allowing to identify multiple objects in a single image.</s><s>These algorithms work by drawing bounding boxes around object targets along with a confidence score, indicating the likelihood that the bounding box contains the object.</s></p><p><s>Currently, object detection models consist of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b23">(Krizhevsky et al., 2012)</ref>, which are typically composed of three main components: the backbone network, the neck, and the head.</s><s>The backbone, commonly a pretrained CNN, extracts and encodes features from the input data; the neck further processes these features, enhancing their representational and informative power.</s><s>One example is Feature Pyramid Network (FPN) <ref type="bibr" target="#b30">(Lin et al., 2019)</ref>.</s><s>Finally, the head predicts the bounding boxes and class probabilities of detected objects based on the previously extracted information.</s></p><p><s>Object detectors can be categorized into two main types depending on their architecture: one-stage and two-stage detectors.</s><s>The former predicts bounding boxes and class probabilities in a single forward pass, while the latter, as the Region-based Convolutional Neural Networks <ref type="bibr" target="#b15">(Girshick et al., 2014)</ref> first proposes regions of interests (ROIs) in the image and then predicts the class and refine the bounding box for each proposed region.</s><s>In our study, we chose to use the latest state-ofthe-art detection architectures: YOLOv8 and Faster R-CNN.</s><s>We selected these algorithms based on their respective strengths and suitability for our specific requirements.</s></p><p><s>For data annotation, we employed the open-source software CVAT (CVAT.ai</s><s>Corporation, 2023).</s><s>Under the guidance of entomologists, we labeled all instances related to the target pests even though their visual appearance could sometimes confuse the detector and introduce noise.</s><s>Annotations were exported in YOLO format, which consists of string lines written as:</s></p><p><s>( c l a s s _ i d x _ b o x _ c e n t r e y _ b o x _ c e n t r e width height)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">YOLO</head><p><s>YOLO (You Only Look Once) is a popular family of one-stage object detection models known for their speed and efficiency <ref type="bibr" target="#b35">(Redmon et al., 2016)</ref>.</s><s>Unlike two-stage methods, YOLO solves detection as a regression problem.</s></p><p><s>Developed by Ultralytics <ref type="bibr">(Glenn Jocher et al., 2023)</ref> and released in January 2023, YOLOv8 serves as the latest advancement in the YOLO family (as of the time of writing).</s><s>It incorporates several improvements, including mosaic data augmentation, anchor-free detection, a more powerful backbone network, a decoupled head, and a modified loss function.</s><s>Among the various model variants, we focused on YOLOv8s due to our computational constraints.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Faster R-CNN</head><p><s>Faster R-CNN implements Region Proposal Network (RPN) for generating potential bounding box proposals and a bounding box regression and classification network for refining these proposals and predicting the class labels <ref type="bibr" target="#b36">(Ren et al., 2016)</ref>.</s><s>We implemented the algorithm using the Detectron2 framework <ref type="bibr" target="#b47">(Wu et al., 2019)</ref>, a cutting-edge tool developed by Facebook for a wide range of computer vision tasks.</s></p><p><s>For our study, we used the faster_rcnn_R_50_FPN_3x.yaml configuration, which uses a ResNet-50 <ref type="bibr" target="#b21">(He et al., 2016)</ref> backbone network and integrates the FPN network to generate multiple feature maps of different scales.</s><s>This configuration provides a good balance between speed and accuracy.</s><s>The "3x" designation refers to the length of the training schedule <ref type="bibr" target="#b20">(He et al., 2018)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experiments and evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Experiment design</head><p><s>We conducted several tests to benchmark the machine vision models, assessing the impact of the different preprocessing steps on their detection capability.</s><s>Initially, we evaluated the effect of image enhancements to understand how it influenced training performance.</s><s>The second test aimed to assess the impact of image resolution, as it is indeed known to significantly affect performance, albeit with a considerable increase in computational cost.</s><s>In our tests, we focused on 640 and 1280 pixel images, both considered reasonable sizes to balance computational time and performance, while avoiding memory constraints.</s><s>The algorithm automatically resized the images, setting their longest dimension to the chosen value, while preserving the original aspect ratio.</s></p><p><s>A similar test was conducted to evaluate the YOLOv8 built-in data augmentation.</s><s>Based on several hyperparameters, default transformations are randomly applied to the training data to increase the diversity and size of the dataset.</s><s>We conducted two training runs to compare the effects of default hyperparameters with their zeroing (see Supplementary Table <ref type="table" target="#tab_0">1</ref>).</s></p><p><s>From a more fundamental perspective, we explored whether, for our dataset, a deep learning model learns better when trained on one class (one insect species) at a time compared to binary-class detection.</s><s>To get more insight on this aspect, we conducted an additional test by considering both classes as a single entity, labeled "pest".</s></p><p><s>Finally, we evaluated the model architectures.</s><s>After implementing Faster R-CNN with three different augmentation settings, we compared the best configuration with the one-stage detector, YOLOv8s.</s></p><p><s>To ensure a more robust estimate of model performance and to allow an honest estimation of the variability of the prediction metrics, we implemented a 10-fold Cross Validation scheme <ref type="bibr" target="#b19">(Hastie et al., 2009)</ref>.</s><s>Given the challenges associated with class stratification in object detection tasks, we selected a random K-fold splitting that achieved an acceptable balance in the distribution of the two classes (Supplementary Table <ref type="table">2</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Evaluation metrics</head><p><s>To assess the performance of an object detection model, we examine its ability to correctly identify the object's class and accurately predict their bounding box coordinates.</s><s>Each prediction is characterized by a value of Intersection over Union (IoU) and confidence score.</s><s>IoU, based on the Jaccard index, evaluates the degree of overlap between the predicted bounding box and the ground truth.</s><s>Values range between 0 to 1, where a value closer to 1 implies a better alignment between the predicted and ground truth bounding boxes.</s><s>The confidence score, instead, indicates the likelihood that the object in the bounding box actually belongs to a specific category.</s><s>Based on these values, correct predictions are classified as True Positives (TP), while False Positives (FP) include detections of nonexistent target objects, which in our case are insects wrongly identified as ST or OI or misplaced detection of existing objects.</s><s>False Negatives (FN) encompass all unpredicted ground truth bounding boxes.</s><s>It's worth noting that True Negatives (TN) are not considered in object detection, as there exists an infinite number of bounding boxes that should not be detected within an image <ref type="bibr" target="#b33">(Padilla et al., 2020)</ref>.</s></p><p><s>From these statistics, we can derive several performance indices, including Precision and Recall.</s><s>Precision (Equation <ref type="formula">1</ref>) measures the model's ability to identify true objects while minimizing the number of incorrect annotations.</s><s>Conversely, Recall (Equation <ref type="formula">2</ref>) focuses on the model's ability to identify all correct objects (TP), regardless of incorrect annotations.</s><s>Ideally, a perfect model would have both high Precision and high Recall.</s><s>For insect detection, we opted for low values of confidence score to make the model generate more predictions.</s><s>This approach results in higher Recall, minimizing FN at the expense of increasing FP <ref type="bibr" target="#b46">(Wenkel et al., 2021)</ref>.</s></p><p><s>Lastly, F1-score (Equation <ref type="formula">3</ref>) shows the harmonic mean of Precision and Recall, considering both FP and FN.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision = TP TP + FP</head><p><s>(1)</s></p><formula xml:id="formula_0">Recall = TP TP + FN (2) F1 score = 2 precision -1 + recall -1</formula><p><s>(3)</s></p><p><s>IoU and precision-recall measures are used to compute Average Precision (AP) (Equation <ref type="formula">4</ref>) for each class.</s><s>By leveraging the area under the precision-recall curve (AUC-PR) and different thresholds of IoU, AP was first estimated using the 11-point interpolation method in the VOC2007 challenge <ref type="bibr" target="#b13">(Everingham et al., 2015)</ref> to reduce the zig-zag behavior of the curve.</s><s>The most common IoU values are 0.5 and 0.75, corresponding to AP@0.5 and AP@0.75, respectively, while AP@[0.5:0.95]</s><s>represents instead the average precision across ten IoU thresholds varying from 0.5 to 0.95 with a step size of 0.05.</s><s>Mean Average Precision (mAP) (Equation <ref type="formula">5</ref>) is then calculated as the mean over all classes, serving as the benchmark metric to evaluate object detection model performance.</s></p><formula xml:id="formula_1">AP@a = Z 1 0 p(r)dr (4) mAP@a = 1 n o n i=1</formula><p><s>AP i for n classes (5)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Experimental setup</head><p><s>Training, validation and inference tests were executed on Amazon Web Services (AWS) virtual machines using a g5.2xlarge instance, which belongs to the GPU instance family.</s><s>It is equipped with 8 vCPUs, 32.0 GiB of memory, and a NVIDIA A10G with 24.0 GiB of video memory.</s><s>The configuration settings for each experiment, partially tuned to comply with hardware constraints, are saved in the corresponding YAML files, which are provided in the Supplementary Material.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p><s>Figure <ref type="figure" target="#fig_3">4</ref> displays examples of model predictions, featuring randomly selected zoomed-in images from the four different sources, along with both ground-truth and predicted annotations.</s><s>Bounding boxes, obtained from one of the YOLOv8s model tests, are displayed with specific class colors and their corresponding confidence scores.</s><s>These photos provide a clear view of the model's performance across various scenarios.</s><s>For instance, in the scanned trap image (Figure <ref type="figure" target="#fig_3">4B</ref>), all insects are detected accurately.</s><s>Challenges arise in Figure <ref type="figure" target="#fig_3">4C</ref>, where the photo presents a dense concentration of OI bounding boxes, making accurate detection more difficult.</s><s>Similarly, in Figure <ref type="figure" target="#fig_3">4D</ref>, the greater distance and ambient light conditions contribute to an increase in both FN and FP.</s></p><p><s>Quantitative results are presented following the experimental workflow, starting with the YOLO algorithm and moving to Faster R-CNN.</s><s>Performance metrics are expressed as the mean and standard deviation across the 10 folds (Table <ref type="table">2</ref>), highlighting the variability of cross-validation splits.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance of YOLO models</head><p><s>Figure <ref type="figure" target="#fig_2">5</ref> summarizes the YOLO experiments on input image modifications and class detection, comparing the three Mean Average Precision (mAP) discussed in section 2.4.2, mAP@0.5, mAP@0.75,</s><s>mAP@[0.5:0.95].</s><s>The Supplementary Material includes the corresponding Precision-Recall curves obtained during validation at the specific confidence thresholds.</s></p><p><s>From the comparison of data enhancements (Figure <ref type="figure" target="#fig_2">5A</ref>), we observe a clear similarity between the Crop and Bright models.</s><s>Results show mAP@0.5 values ranging from 0.9 to 0.95, mAP@0.75</s><s>above 0.8, and mAP@[0.5:0.95] between 0.65 and 0.7, with a difference of less than 2% in the other metrics (see Table <ref type="table">2</ref>).</s><s>On the other hand, Sharp and Bright_and_sharp models achieved slightly lower results and higher variabilities, with all mAP values dropping by up to 3%, especially the latter.</s></p><p><s>The impact of input image resizing (Figure <ref type="figure" target="#fig_2">5B</ref>) is more pronounced, particularly in terms of mAP@0.75 and Recall (Table <ref type="table">2</ref>).</s><s>This observation suggests that image resolution becomes increasingly critical when higher IoU thresholds are required or for the complete detection of ground-truth annotations.</s></p><p><s>Figure <ref type="figure" target="#fig_2">5C</ref> clearly demonstrates the effect of data augmentation during training.</s><s>Without transformations, the model did not exceed 0.81 in mAP@0.5, 0.67 in mAP@0.75, and 0.57 in mAP@[0.5:0.95],</s></p><p><s>with a low Recall of 72%.</s><s>Training with data augmentation drastically improves these metrics, particularly mAP@0.75 and Recall (Table <ref type="table">2</ref>).</s></p><p><s>Regarding class detection tests, Figure <ref type="figure" target="#fig_2">5D</ref> shows no significant difference between the Average Precision (AP) of ST for both the binary-class and single-class models.</s><s>The same holds true of the OI class (Figure <ref type="figure" target="#fig_2">5E</ref>), with subtle differences of less than 2%, except for a 4% increase in Recall for the binary-class model (Table <ref type="table">2</ref>).</s><s>Finally, the last plot compares the mAP of the binary-class model and the AP of the pest-class model, both achieving high results: 0.92 mAP@ 0.5, 0.8 mAP@0.75, and 0.66 mAP@0.5:0.95, with 90% Precision, 87% Recall, and 88% F1 score as shown in Table <ref type="table">2</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance of Faster R-CNN models</head><p><s>Faster R-CNN results include an assessment of mAP performance when changing the augmentation settings.</s><s>Three tests were conducted, named Default, Augmentation, and No_augmentation, each based on specific transformations applied during training, similarly to the YOLOv8 built-in data augmentation.</s><s>For further details on the code, please refer to our GitHub repository, <ref type="url" target="https://github.com/checolag/insect-detection-scripts">https://github.com/checolag/insect-detection-scripts</ref>.</s></p><p><s>Since Detectron2 does not provide Precision and Recall metrics, we monitored the progress of mAP over iterations for the three tests, as depicted in Figure <ref type="figure" target="#fig_5">6</ref>.</s><s>From the graph, we observed two main trends: the default configuration notably outperforms the model with augmentation, and maximum values are generally reached within the first 1500 iterations, after which they remain relatively constant.</s></p><p><s>As Detectron2 only saves the last model and not the best one, the metric values in Table <ref type="table">2</ref> were derived considering the iteration at which the model of each split achieved the best results in terms of mAP@0.5, mAP@0.75, and mAP@[0.5:0.95].</s><s>Although the differences are small, they are relevant, with the default run reaching 86% in mAP@0.5, 66% in mAP@0.75, and 55% in mAP@[0.5:0.95].</s></p><p><s>TABLE 2 Insect detection performance of the 8 tests conducted.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head><p><s>Test Configuration mAP@0.5 mAP@0.75</s><s>mAP@0.</s></p><p><s>5:0.95 Precision Recall F1 score 1 Data enhancement Crop 0.92 ± 0.02 0.81 ± 0.03 0.66 ± 0.02 0.89 ± 0.05 0.89 ± 0.03 0.89 ± 0.03 Bright 0.92 ± 0.04 0.79 ± 0.04 0.66 ± 0.03 0.90 ± 0.03 0.87 ± 0.06 0.89 ± 0.04 Sharp 0.90 ± 0.04 0.78 ± 0.04 0.64 ± 0.03 0.89 ± 0.04 0.85 ± 0.04 0.87 ± 0.04 Bright and sharp 0.89 ± 0.04 0.77 ± 0.05 0.63 ± 0.03 0.87 ± 0.04 0.83 ± 0.06 0.85 ± 0.04 2 Input image size Crop 640 0.88 ± 0.04 0.71 ± 0.05 0.60 ± 0.03 0.87 ± 0.03 0.82 ± 0.05 0.84 ± 0.04 Crop 1280 0.92 ± 0.02 0.81 ± 0.03 0.66 ± 0.02 0.89 ± 0.05 0.89 ± 0.03 0.89 ± 0.03 3 Data augmentation no augmentation 0.81 ± 0.07 0.67 ± 0.08 0.56 ± 0.06 0.86 ± 0.03 0.72 ± 0.12 0.78 ± 0.08 default augmentation 0.92 ± 0.02 0.81 ± 0.03 0.66 ± 0.02 0.89 ± 0.05 0.89 ± 0.03 0.89 ± 0.03 4 Single class ST Binary class* 0.94 ± 0.02 0.83 ± 0.04 0.68 ± 0.03 0.91 ± 0.03 0.90 ± 0.04 0.90 ± 0.03 Single class 0.93 ± 0.03 0.82 ± 0.05 0.67 ± 0.03 0.92 ± 0.03 0.90 ± 0.04 0.91 ± 0.03 5 Single class OI Binary class* 0.91 ± 0.04 0.79 ± 0.05 0.65 ± 0.04 0.88 ± 0.08 0.88 ± 0.06 0.87 ± 0.05 Single class 0.90 ± 0.07 0.77 ± 0.06 0.64 ± 0.05 0.89 ± 0.08 0.84 ± 0.08 0.86 ± 0.07 6 Mono class pest binary class 0.92 ± 0.02 0.81 ± 0.03 0.66 ± 0.02 0.89 ± 0.05 0.89 ± 0.03 0.89 ± 0.03 single_cls=True 0.92 ± 0.03 0.80 ± 0.03 0.66 ± 0.02 0.90 ± 0.02 0.87 ± 0.05 0.88 ± 0.03 7 Data augmentation in Detectron2 Default Trainer 0.86 ± 0.06 0.66 ± 0.07 0.55 ± 0.05 --no Flip transformation 0.84 ± 0.07 0.64 ± 0.07 0.53 ± 0.05 ---Augmentation 0.83 ± 0.05 0.58 ± 0.07 0.50 ± 0.04 ---8 Model architecture Yolov8s 0.92 ± 0.02 0.81 ± 0.03 0.66 ± 0.02 0.89 ± 0.05 0.89 ± 0.03 0.89 ± 0.03 Faster R-CNN 0.86 ± 0.06 0.66 ± 0.07 0.55 ± 0.05 ---The metrics include the mean and standard deviation values between 0 and 1 among the 10 folds.</s><s>*To compare the test with the single-class training, we show the corresponding Average Precision (AP) value of the binary-class training.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison of the algorithms</head><p><s>This section concludes the experimental evaluation of model architectures, highlighting the difference between the optimal configurations of Faster R-CNN and YOLOv8 that can be computationally managed by our hardware system.</s><s>Specifically, we compare the chosen Faster R-CNN version with default augmentation settings against YOLOv8 with an input size of 1280 pixels and default augmentation hyperparameters.</s><s>Both models were trained using only cropped input images.</s></p><p><s>The boxplots in Figure <ref type="figure">7</ref> illustrate mAP@0.5, mAP@0.75, and mAP@[0.5:0.95]</s><s>across the 10 folds of cross-validation.</s><s>We observe how YOLOv8s outperforms Faster R-CNN in terms of both accuracy and robustness.</s><s>The percentage difference between the average values exceeds 6% in mAP@0.5, 15% in mAP@0.75, and more than 10% in mAP@[0.50:0.95].</s><s>Moreover, the size of the boxplot clearly shows the higher prediction variability of Faster R-CNN compared to YOLOv8.</s><s>As shown in Table <ref type="table">2</ref>, the performance of the two-stage algorithm is highly dependent on the validation split, with a standard deviation of about 6%, compared to YOLOv8's 2%.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p><s>Building on the previously mentioned work on ST detection <ref type="bibr" target="#b2">(Bessa, 2021)</ref>, which our study aims to expand, this benchmark has demonstrated the effectiveness of object detection algorithms in recognizing ST and OI on yellow sticky traps.</s><s>We showed how a standardized acquisition procedure,particularly in the scanned imagescombined with a color segmentation, can achieve strong detection performance.</s><s>Automated detection of FD vectors has proven both feasible and effective, supporting essential pest management strategies against the spread of this grapevine disease <ref type="bibr" target="#b26">(Lee and Tardaguila, 2023)</ref>.</s></p><p><s>Interestingly, the first test revealed that enhancing sharpness did not improve the model performance.</s><s>This modification appeared to introduce noise to the image, which the model interpreted as irrelevant information.</s><s>Conversely, variations in brightness and contrast resulted in similar detection accuracy as the non-processed dataset, suggesting that the original dataset was already suitable for training, and additional changes did not provide any further benefits.</s><s>Further studies should be conducted to understand the relationship between model architecture and image processing, with the aim of optimizing the model training process.</s><s>The use of higher resolution images significantly improved mAP values, with more pronounced effects observed at higher IoU values.</s><s>However, in case of limited computational resources, an image size of 640 pixels proved to be a good compromise between accuracy and computing time.</s><s>In accordance with a similar study <ref type="bibr" target="#b10">(Dang et al., 2023)</ref>, YOLOv8 built-in augmentation resulted in actual improvements, further demonstrating the effectiveness of the default hyperparameter settings.</s></p><p><s>Class-oriented tests revealed that single-class detectors did not perform better than multi-class models.</s><s>No significant changes in terms of TP, FP and FN were noted when trained on one class at a time; in fact, they obtained equal or lower results, as seen with the OI class.</s><s>Even when the classes were combined under a single target label, the differences were minimal.</s><s>This suggests that binary-class training is a viable strategy for maximizing performance and feedback information.</s><s>The reason why the ST class achieves higher results and lower variability than OI could be attributed to the distribution of annotations.</s><s>ST labels are primarily concentrated in one source of image, i.e. the scanned images, which constitutes the majority of the dataset.</s><s>In contrast, OI annotations were present in all image types, which vary significantly from each other.</s><s>Some images contain dense clusters of labels, making the detection more challenging.</s></p><p><s>The Faster R-CNN tests yielded unexpected results.</s><s>Adding several transformations appeared to confuse the model, resulting in lower mAP values, especially for higher IoU values.</s><s>While augmentation is typically beneficial, enabling the model to learn under various situations such as different lighting, orientations, distortions, and variations in object sizes and shapes, in this specific case, these random modifications during training only had negative effects.</s><s>A possible explanation could be a mismatch with real-world data, as the augmentations might not accurately reflect the variability present in actual scenarios.</s><s>Training curves of mAP@0.5, mAP@0.75, and mAP@[0.5:0.95] for the three augmentation tests with the Faster R-CNN algorithm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIGURE 7</head><p><s>Comparison of mAP@0.5, mAP@0.75, and mAP@[0.5:0.95]</s><s>values between the optimal configuration of Faster R-CNN and YOLOv8s.</s></p><p><s>Lastly, the superior performance of YOLOv8 over the two-stage detector is consistent with findings from other recent studies <ref type="bibr" target="#b7">(Butt et al., 2024)</ref>.</s><s>This could be attributed to the more recent advancements in the YOLO model, making it better equipped for our specific detection task.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Limitations</head><p><s>Despite the advancements in computer vision and deep learning techniques, insect detection remains a highly challenging task.</s><s>One major obstacle is the limited availability of data essential for model training, necessitating the construction of our own insect dataset.</s><s>Depending on the type of study, target objects can be exceedingly small, difficult to see, and may exhibit variability in terms of shape, color, wing poses, and decay conditions <ref type="bibr" target="#b25">(Le et al., 2021)</ref>, adding complexity to the creation of a robust and consistent dataset.</s><s>Moreover, the acquisition process in an uncontrolled environment introduces various other forms of noise, including reflections, shadows, orientations, blurring, and variations in visual appearance.</s></p><p><s>As discussed in section 4, annotations in our study were not uniformly distributed across the dataset, particularly for the OI class, with labels concentrated in fewer densely populated images.</s><s>Additionally, the condition of insects was often very compromised, potentially introducing noise and affecting model training.</s><s>In this regard, we opted to label everything potentially related to the specific pests, despite the risk of increasing the number of false positives (such as misidentifying dry leaves as the ST class).</s></p><p><s>Finally, another factor to consider is the presence of other insects on the yellow traps, particularly other Cicadellidae species that closely resemble S. titanus, which may be erroneously identified by the model.</s><s>Notable examples include Fieberiella florii and Phlogotettix cyclops, as highlighted in previous studies <ref type="bibr" target="#b6">(Bosco et al., 1997;</ref><ref type="bibr" target="#b8">Chuche et al., 2010;</ref><ref type="bibr" target="#b40">Strauss and Reisenzein, 2018)</ref>.</s><s>To address this issue, our strategy involves the collection of digitized trap images.</s><s>This simple yet efficient approach allows us to continuously enrich the dataset over time.</s><s>By progressively incorporating more data, we can enhance the model's capability to distinguish between highly similar species.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p><s>This study was initiated to evaluate the latest deep learning models for insect detection, aiming to take a significant step forward in the control of Flavescence doreé.</s><s>The collected images constitute the first fully annotated dataset of Scaphoideus titanus and Orientus ishidae, which is now available to the scientific community (see Data availability section) and can be expanded over time focusing on a standardized and reproducible procedure.</s><s>We trained deep learning models using YOLOv8 and Faster R-CNN architectures and conducted a benchmark analysis, providing valuable insights and operational tips for acquisition, augmentation and training processes.</s><s>The two algorithms achieved mAP@0.5 scores of 0.92 and 0.86 respectively, demonstrating the effectiveness of object detectors in addressing this challenging problem.</s><s>Moving forward, possible improvements could involve optimizing the acquisition process, enhancing image quality, and adding location details to track vector spread in vineyards.</s><s>Additionally, a segmentation model could simplify field data acquisition by automatically cropping yellow traps before applying subsequent operations.</s><s>The deployment of these models could establish an efficient monitoring network, opening up potential applications for field-use scenarios.</s><s>Specifically, a smartphone tool capable of identifying FD vectors would not only enable farmers to take immediate action against the disease, but also allow the scientific community to continuously update the dataset, in the spirit of citizen science.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1</head><label>1</label><figDesc><div><p><s>FIGURE 1 Samples of insect vectors captured by yellow sticky traps.</s><s>The first row represents examples of the ST class, while the second row shows examples of the OI class.</s></p></div></figDesc><graphic coords="2,99.38,538.07,396.85,189.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2</head><label>2</label><figDesc><div><p><s>FIGURE 2</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 5</head><label>5</label><figDesc><div><p><s>FIGURE 5 mAP evaluation of YOLO experiments: (A-C) represent the comparison on input image modifications; (D-F) include the class-oriented tests.</s></p></div></figDesc><graphic coords="7,106.47,520.27,382.68,216.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 4</head><label>4</label><figDesc><div><p><s>FIGURE 4 Examples of zoomed insect images with predicted bounding boxes.</s><s>Red and pink colors represent respectively the detections of ST and OI classes.</s><s>(A) photos from the smart trap; (B) details from scanned trap images; (C) photos in the laboratory; (D) pictures from the field.</s></p></div></figDesc><graphic coords="7,99.38,88.84,396.79,199.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><figDesc><div><p><s>Default: This test used the two default Detectron2 transformations, ResizeShortestEdge and RandomFlip.</s><s>The first resizes the image while keeping the aspect ratio, while the other operation flips the image horizontally or vertically with a given probability; • Augmentation: This test introduced additional D e t e c t r o n 2 t r a n s f o r m a t i o n s , i n c l u d i n g R a n d o m B r i g h t n e s s , R a n d o m C o n t r a s t , R a n d o m S a t u r a t i o n , R a n d o m R o t a t i o n , RandomLighting, along with ResizeShortestEdge and RandomFlip.</s><s>These transformations randomly alter the intensity of image enhancements during training to augment the diversity of the training data; • No_augmentation: This test represented the default training configuration without applying RandomFlip to input images.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 6</head><label>6</label><figDesc><div><p><s>FIGURE 6</s></p></div></figDesc><graphic coords="9,60.83,88.84,226.77,138.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc><div><p><s>Structure of the dataset, showing the number of images from each data source and the corresponding class annotations.</s></p></div></figDesc><table><row><cell>Image source</cell><cell>Number of images</cell><cell>ST annotations</cell><cell>OI annotations</cell><cell>Number of background images</cell></row><row><cell>Field</cell><cell>18</cell><cell>3</cell><cell>101</cell><cell>8</cell></row><row><cell>Laboratory</cell><cell>157</cell><cell>473</cell><cell>863</cell><cell>8</cell></row><row><cell>scanned</cell><cell>390</cell><cell>853</cell><cell>542</cell><cell>84</cell></row><row><cell>smart-trap</cell><cell>50</cell><cell>0</cell><cell>0</cell><cell>50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p><s>Checola et al.  10.3389/fpls.2024.1485216</s><s>Frontiers in Plant Science frontiersin.org</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p><s>Frontiers in Plant Science frontiersin.org</s></p></note>
		</body>
		<back>

			<div type="funding">
<div><head>Funding</head><p><s>The author(s) declare financial support was received for the research, authorship, and/or publication of this article.</s><s>This study was carried out within the <rs type="funder">Interconnected Nord-Est Innovation Ecosystem (iNEST)</rs> and received funding from the <rs type="funder">European Union Next-GenerationEU</rs> (<rs type="grantNumber">PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) -MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.5 -D.D. 1058 23/06/2022</rs>, <rs type="grantNumber">ECS00000043</rs>).</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jBG8Gbr">
					<idno type="grant-number">PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) -MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.5 -D.D. 1058 23/06/2022</idno>
				</org>
				<org type="funding" xml:id="_KdkfdCn">
					<idno type="grant-number">ECS00000043</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability statement</head><p><s>The dataset presented in the study is publicly available at <ref type="url" target="https://zenodo.org/records/11441757">https://zenodo.org/records/11441757</ref>.</s><s>For further details on the code, please refer to our GitHub repository, <ref type="url" target="https://github.com/checolag/insect-detection-scripts">https://github.com/  checolag/insect-detection-scripts</ref>.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics statement</head><p><s>The manuscript presents research on animals that do not require ethical approval for their study.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p><s>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</s></p><p><s>The author(s) declared that they were an editorial board member of Frontiers, at the time of submission.</s><s>This had no impact on the peer review process and the final decision.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's note</head><p><s>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers.</s><s>Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author disclaimer</head><p><s>This manuscript reflects only the authors' views and opinions, neither the European Union nor the European Commission can be considered responsible for them.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p><s>The Supplementary Material for this article can be found online at: <ref type="url" target="https://www.frontiersin.org/articles/10.3389/fpls.2024.1485216/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fpls.2024.1485216/  full#supplementary-material</ref></s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning based detector YOLOv5 for identifying insect pests</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.3390/app121910167</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">10167</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahmad, I., Yang, Y., Yue, Y., Ye, C., Hassan, M., Cheng, X., et al. (2022). Deep learning based detector YOLOv5 for identifying insect pests. Appl. Sci. 12, 10167. doi: 10.3390/app121910167</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">New insights in phytoplasma-vector interaction: acquisition and inoculation of flavescence doreé phytoplasma by Scaphoideus titanus adults in a short window of time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lessio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picciau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandrioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tota</surname></persName>
		</author>
		<idno type="DOI">10.1111/aab.12433</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Biol</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="55" to="62" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alma, A., Lessio, F., Gonella, E., Picciau, L., Mandrioli, M., and Tota, F. (2018). New insights in phytoplasma-vector interaction: acquisition and inoculation of flavescence doreé phytoplasma by Scaphoideus titanus adults in a short window of time. Ann. Appl. Biol. 173, 55-62. doi: 10.1111/aab.12433</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automatic processing of images of chromotropic traps for identification and quantification of Trioza erytreae and Scaphoideus titanus</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L D S P</forename><surname>Bessa</surname></persName>
		</author>
		<ptr target="https://oasisbr.ibict.br/vufind/Record/RCAP_90853832dd" />
		<imprint>
			<date type="published" when="2021-12-19">2021. 3730559f83110e0bf17a2d. Accessed December 19, 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bessa, B. L. d. S. P. (2021). Automatic processing of images of chromotropic traps for identification and quantification of Trioza erytreae and Scaphoideus titanus. Available online at: https://oasisbr.ibict.br/vufind/Record/RCAP_90853832dd3730559f83110e0bf17a2d (Accessed December 19, 2023).</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate detection and identification of insects from camera trap images with deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bjerge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dyrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Frigaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Høye</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pstr.0000051</idno>
	</analytic>
	<monogr>
		<title level="j">PloS Sustain. Transform</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bjerge, K., Alison, J., Dyrmann, M., Frigaard, C. E., Mann, H. M. R., and Høye, T. T. (2023). Accurate detection and identification of insects from camera trap images with deep learning. PloS Sustain. Transform. 2, e0000051. doi: 10.1371/journal.pstr.0000051</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time insect tracking and monitoring with computer vision and deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bjerge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Høye</surname></persName>
		</author>
		<idno type="DOI">10.1002/rse2.245</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Ecol. Conserv</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="315" to="327" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bjerge, K., Mann, H. M. R., and Høye, T. T. (2022). Real-time insect tracking and monitoring with computer vision and deep learning. Remote Sens. Ecol. Conserv. 8, 315-327. doi: 10.1002/rse2.245</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An automated light trap to monitor moths (Lepidoptera) using computer vision-based tracking and deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bjerge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Sepstrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Helsing-Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Høye</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21020343</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">343</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bjerge, K., Nielsen, J. B., Sepstrup, M. V., Helsing-Nielsen, F., and Høye, T. T. (2021). An automated light trap to monitor moths (Lepidoptera) using computer vision-based tracking and deep learning. Sensors 21, 343. doi: 10.3390/s21020343</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Studies on population dynamics and spatial distribution of leafhoppers in vineyards (Homoptera: Cicadellidae)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arzone</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1744-7348.1997.tb05778.x</idno>
	</analytic>
	<monogr>
		<title level="m">The openCV library</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1997">1997. 2000</date>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="122" to="125" />
		</imprint>
	</monogr>
	<note>Ann. Appl. Biol.</note>
	<note type="raw_reference">Bosco, D., Alma, A., and Arzone, A. (1997). Studies on population dynamics and spatial distribution of leafhoppers in vineyards (Homoptera: Cicadellidae). Ann. Appl. Biol. 130, 1-11. doi: 10.1111/j.1744-7348.1997.tb05778.x Bradski, G. (2000). The openCV library. Dr Dobb&apos;s J. Software Tools 120, 122-125.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Application of YOLOv8 and detectron2 for bullet hole detection and score calculation from shooting cards</title>
		<author>
			<persName><forename type="first">M</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Glas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Monsuur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Keijzer</surname></persName>
		</author>
		<idno type="DOI">10.3390/ai5010005</idno>
	</analytic>
	<monogr>
		<title level="j">AI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="72" to="90" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Butt, M., Glas, N., Monsuur, J., Stoop, R., and de Keijzer, A. (2024). Application of YOLOv8 and detectron2 for bullet hole detection and score calculation from shooting cards. AI 5, 72-90. doi: 10.3390/ai5010005</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">First description of the occurrence of the leafhopper Phlogotettix cyclops in a Bordeaux vineyard</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chuche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Danet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thieŕy</surname></persName>
		</author>
		<idno type="DOI">10.20870/oeno-one.2010.44.2.1467</idno>
		<ptr target="https://github.com/cvat-ai/cvat" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision Annotation Tool (CVAT). Available online at</title>
		<imprint>
			<date type="published" when="2010">2010. 2023. May 30, 2024</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">161</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Chuche, J., Danet, J.-L., and Thieŕy, D. (2010). First description of the occurrence of the leafhopper Phlogotettix cyclops in a Bordeaux vineyard. OENO One 44, 161. doi: 10.20870/oeno-one.2010.44.2.1467 CVAT.ai Corporation (2023). Computer Vision Annotation Tool (CVAT). Available online at: https://github.com/cvat-ai/cvat (Accessed May 30, 2024).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Orientus ishidae (Hemiptera: cicadellidae): biology, direct damage and preliminary studies on apple proliferation infection in apple orchard</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dalmaso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ioriatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gualandri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zapponi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mori</surname></persName>
		</author>
		<idno type="DOI">10.3390/insects14030246</idno>
	</analytic>
	<monogr>
		<title level="j">Insects</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">246</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dalmaso, G., Ioriatti, C., Gualandri, V., Zapponi, L., Mazzoni, V., Mori, N., et al. (2023). Orientus ishidae (Hemiptera: cicadellidae): biology, direct damage and preliminary studies on apple proliferation infection in apple orchard. Insects 14, 246. doi: 10.3390/insects14030246</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">YOLOWeeds: A novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compag.2023.107655</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page">107655</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dang, F., Chen, D., Lu, Y., and Li, Z. (2023). YOLOWeeds: A novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems. Comput. Electron. Agric. 205, 107655. doi: 10.1016/j.compag.2023.107655</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic moth detection from trap images for pest management</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compag.2016.02.003</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="2016">2016</date>
			<publisher>European Food Safety Authority</publisher>
			<pubPlace>Tramontini, S., Delbianco, A., Vos, S</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Ding, W., and Taylor, G. (2016). Automatic moth detection from trap images for pest management. Comput. Electron. Agric. 123, 17-28. doi: 10.1016/j.compag.2016.02.003 EFSA (European Food Safety Authority), Tramontini, S., Delbianco, A., Vos, S.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pest survey card on flavescence doreé phytoplasma and its vector Scaphoideus titanus</title>
		<idno type="DOI">10.2903/sp.efsa.2020.EN-1909</idno>
	</analytic>
	<monogr>
		<title level="j">EFSA Support. Publ</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1909">1909</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pest survey card on flavescence doreé phytoplasma and its vector Scaphoideus titanus. EFSA Support. Publ. 17, 1909E. doi: 10.2903/sp.efsa.2020.EN-1909</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-014-0733-5</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C. K. I., Winn, J., and Zisserman, A. (2015). The pascal visual object classes challenge: A retrospective. Int. J. Comput. Vis. 111, 98-136. doi: 10.1007/s11263-014-0733-5</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">First detection of the mosaic leafhopper, Orientus ishidae, in northern Italian vineyards infected by the flavescence doreé phytoplasma</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gaffuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cavagna</surname></persName>
		</author>
		<idno type="DOI">10.5197/j.2044-0588.2011.024.022</idno>
	</analytic>
	<monogr>
		<title level="j">New Dis. Rep</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="22" to="22" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gaffuri, F., Sacchi, S., and Cavagna, B. (2011). First detection of the mosaic leafhopper, Orientus ishidae, in northern Italian vineyards infected by the flavescence doreé phytoplasma. New Dis. Rep. 24, 22-22. doi: 10.5197/j.2044- 0588.2011.024.022</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.81</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
	<note type="raw_reference">Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). &quot;Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,&quot; in Proceedings of the IEEE conference on computer vision and pattern recognition (IEEE Computer Society), 580- 587. doi: 10.1109/CVPR.2014.81</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carlos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Goncalves, J., Silva, E., Faria, P., Nogueira, T., Ferreira, A., Carlos, C., et al. (2022).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Edge-compatible deep learning models for detection of pest outbreaks in viticulture</title>
		<idno type="DOI">10.3390/agronomy12123052</idno>
	</analytic>
	<monogr>
		<title level="j">Agronomy</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3052</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Edge-compatible deep learning models for detection of pest outbreaks in viticulture. Agronomy 12, 3052. doi: 10.3390/agronomy12123052</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><surname>Gonella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Benelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arricau-Bouvery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Duso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Dietrich</surname></persName>
		</author>
		<idno type="DOI">10.1127/entomologia/2023/2597</idno>
	</analytic>
	<monogr>
		<title level="m">Scaphoideus titanus up-to-the-minute: biology, ecology, and role as a vector</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="481" to="496" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gonella, E., Benelli, G., Arricau-Bouvery, N., Bosco, D., Duso, C., Dietrich, C. H., et al. (2024). Scaphoideus titanus up-to-the-minute: biology, ecology, and role as a vector. Entomol. Gen. 44, 481-496. doi: 10.1127/entomologia/2023/2597</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: data mining, inference, and prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-84858-7#bibliographic-information</idno>
		<ptr target="https://link.springer.com/book/10.1007/978-0-387-84858-7#bibliographic-information" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Hastie, T., Tibshirani, R., Friedman, J. H., and Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. (New York, NY: Springer). Available online at: https://link.springer.com/book/10.1007/978-0-387-84858- 7#bibliographic-information.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking ImageNet Pre-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollaŕ</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/9010930" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">He, K., Girshick, R., and Dollaŕ, P. (2018). &quot;Rethinking ImageNet Pre-training.&quot; in 2019 IEEE/CVF International Conference on Computer Vision (ICCV). (IEEE). Available online at: https://ieeexplore.ieee.org/document/9010930.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="raw_reference">He, K., Zhang, X., Ren, S., and Sun, J. (2016). &quot;Deep Residual Learning for Image Recognition,&quot; in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE, Las Vegas, NV, USA), 770-778. doi: 10.1109/CVPR.2016.90</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://github.com/ultralytics/ultralytics" />
		<imprint>
			<date type="published" when="2023-04-15">2023. April 15, 2024</date>
		</imprint>
	</monogr>
	<note>Ultralytics YOLO. Available online at</note>
	<note type="raw_reference">Jocher, G., Chaurasia, A., and Qiu, J. (2023). Ultralytics YOLO. Available online at: https://github.com/ultralytics/ultralytics (Accessed April 15, 2024).</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012-05-08">2012. 2012. May 8, 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012).ImageNet Classification with Deep Convolutional Neural Networks. In: Advances in Neural Information Processing Systems (Curran Associates, Inc). Available online at: https://papers.nips.cc/paper_ files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html (Accessed May 8, 2024).</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">YOLO-based light-weight deep learning models for insect detection system with field adaption</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Nagarathna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Flammini</surname></persName>
		</author>
		<idno type="DOI">10.3390/agriculture13030741</idno>
	</analytic>
	<monogr>
		<title level="j">Agriculture</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">741</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kumar, N., Nagarathna,, and Flammini, F. (2023). YOLO-based light-weight deep learning models for insect detection system with field adaption. Agriculture 13, 741. doi: 10.3390/agriculture13030741</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">AlertTrap: A study on object detection in remote insects trap monitoring system using on-the-edge deep learning platform</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Vo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13341</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Le, A. D., Pham, D. A., Pham, D. T., and Vo, H. B. (2021). AlertTrap: A study on object detection in remote insects trap monitoring system using on-the-edge deep learning platform. arXiv preprint arXiv:2112.13341.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pest and Disease Management</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tardaguila</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-26941-7_5</idno>
	</analytic>
	<monogr>
		<title level="m">Advanced Automation for Tree Fruit Orchards and Vineyards</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Vougioukas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="93" to="118" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lee, W. S., and Tardaguila, J. (2023). &quot;Pest and Disease Management,&quot; in Advanced Automation for Tree Fruit Orchards and Vineyards, eds. S. G. Vougioukas and Q. Zhang (Cham: Springer International Publishing), 93-118. doi: 10.1007/978-3-031- 26941-7_5</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Development, spatial distribution, and presence on grapevine of nymphs of orientus ishidae (Hemiptera: cicadellidae), a new vector of flavescence doreé phytoplasmas</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lessio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alma</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1093/jee/toz177</idno>
	</analytic>
	<monogr>
		<title level="j">J. Econ. Entomol</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="2558" to="2564" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lessio, F., Bocca, F., and Alma, A. (2019). Development, spatial distribution, and presence on grapevine of nymphs of orientus ishidae (Hemiptera: cicadellidae), a new vector of flavescence doreé phytoplasmas. J. Econ. Entomol. 112, 2558-2564. doi: 10.1093/jee/toz177</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The mosaic leafhopper Orientus ishidae: host plants, spatial distribution, infectivity, and transmission of 16SrV phytoplasmas to vines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lessio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picciau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandrioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alma</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Insectol</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="277" to="289" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lessio, F., Picciau, L., Gonella, E., Mandrioli, M., Tota, F., and Alma, A. (2016). The mosaic leafhopper Orientus ishidae: host plants, spatial distribution, infectivity, and transmission of 16SrV phytoplasmas to vines. Bull. Insectol. 69, 277-289.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tracking the dispersion of Scaphoideus titanus Ball (Hemiptera: Cicadellidae) from wild to cultivated grapevine: use of a novel markcapture technique</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lessio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alma</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1017/S0007485314000030</idno>
	</analytic>
	<monogr>
		<title level="j">Bull. Entomol. Res</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="432" to="443" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lessio, F., Tota, F., and Alma, A. (2014). Tracking the dispersion of Scaphoideus titanus Ball (Hemiptera: Cicadellidae) from wild to cultivated grapevine: use of a novel mark- capture technique. Bull. Entomol. Res. 104, 432-443. doi: 10.1017/S0007485314000030</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpls.2019.00155</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lin, K., Gong, L., Huang, Y., Liu, C., and Pan, J. (2019). Deep learning-based segmentation and quantification of cucumber powdery mildew using convolutional neural network. Front. Plant Sci. 10. doi: 10.3389/fpls.2019.00155</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Epidemiology of flavescence doreé in vineyards in northwestern Italy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giosuè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Scapin</surname></persName>
		</author>
		<idno type="DOI">10.1094/PHYTO-97-11-1422</idno>
	</analytic>
	<monogr>
		<title level="j">Phytopathology</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1422" to="1427" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Morone, C., Boveri, M., Giosuè, S., Gotta, P., Rossi, V., Scapin, I., et al. (2007). Epidemiology of flavescence doreé in vineyards in northwestern Italy. Phytopathology 97, 1422-1427. doi: 10.1094/PHYTO-97-11-1422</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time detection and classification of scirtothrips dorsalis on fruit crops with smartphone-based deep learning system: preliminary results</title>
		<author>
			<persName><forename type="first">G</forename><surname>Niyigena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.3390/insects14060523</idno>
	</analytic>
	<monogr>
		<title level="j">Insects</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">523</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Niyigena, G., Lee, S., Kwon, S., Song, D., and Cho, B.-K. (2023). Real-time detection and classification of scirtothrips dorsalis on fruit crops with smartphone-based deep learning system: preliminary results. Insects 14, 523. doi: 10.3390/insects14060523</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on performance metrics for object-detection algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Netto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A B</forename><surname>Da Silva</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/9145130" />
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Systems, Signals and Image Processing (IWSSIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="237" to="242" />
		</imprint>
	</monogr>
	<note type="raw_reference">Padilla, R., Netto, S. L., and da Silva, E. A. B. (2020). &quot;A survey on performance metrics for object-detection algorithms,&quot; in 2020 International Conference on Systems, Signals and Image Processing (IWSSIP). (IEEE), 237-242. Available online at: https://ieeexplore.ieee. org/document/9145130.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A real-time object detection model for orchard pests based on improved YOLOv4 algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-022-17826-4</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13557</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pang, H., Zhang, Y., Cai, W., Li, B., and Song, R. (2022). A real-time object detection model for orchard pests based on improved YOLOv4 algorithm. Sci. Rep. 12, 13557. doi: 10.1038/s41598-022-17826-4</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Standardization and criticism of sampling procedures using sticky card traps: monitoring sap-sucking insect pests and Anagrus atomus inhabiting European vineyards</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cargnus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tacoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zandigiacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1506.02640</idno>
		<ptr target="http://www.bulletinofinsectology.org/pdfarticles/vol74-2021-291-306pavan.pdf" />
	</analytic>
	<monogr>
		<title level="j">Bull. Insectol</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="779" to="788" />
			<date type="published" when="2016">2021. 2016</date>
		</imprint>
	</monogr>
	<note>You only look once: unified, real-time object detection</note>
	<note type="raw_reference">Pavan, F., Cargnus, E., Tacoli, F., and Zandigiacomo, P. (2021). Standardization and criticism of sampling procedures using sticky card traps: monitoring sap-sucking insect pests and Anagrus atomus inhabiting European vineyards. Bull. Insectol. 74, 291-306. Available online at: http://www.bulletinofinsectology.org/pdfarticles/vol74-2021-291-306pavan.pdf. Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You only look once: unified, real-time object detection. Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 779-788. doi: 10.48550/arXiv.1506.02640</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1506.01497</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ren, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: towards real-time object detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell. 39.6, 1137-1149. doi: 10.48550/arXiv.1506.01497</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning object detection for image analysis of cherry fruit fly (Rhagoletis cerasi L.) on yellow sticky traps</title>
		<author>
			<persName><forename type="first">C</forename><surname>Salamut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kohnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Landwehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pflanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schirrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zare</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10343-022-00794-0</idno>
	</analytic>
	<monogr>
		<title level="j">Gesunde Pflanz</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Salamut, C., Kohnert, I., Landwehr, N., Pflanz, M., Schirrmann, M., and Zare, M. (2023). Deep learning object detection for image analysis of cherry fruit fly (Rhagoletis cerasi L.) on yellow sticky traps. Gesunde Pflanz. 75, 37-48. doi: 10.1007/s10343-022- 00794-0</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic detection of Flavescense Doreé grapevine disease in hyperspectral images using machine learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nepaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Padua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sousa</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2021.11.081</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="125" to="132" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Silva, D. M., Bernardin, T., Fanton, K., Nepaul, R., Padua, L., Sousa, J. J., et al. (2022). Automatic detection of Flavescense Doreé grapevine disease in hyperspectral images using machine learning. Proc. Comput. Sci. 196, 125-132. doi: 10.1016/ j.procs.2021.11.081</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Insect Detect: An open-source DIY camera trap for automated insect monitoring</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sittinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herz</surname></persName>
		</author>
		<idno type="DOI">10.1101/2023.12.05.570242</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sittinger, M., Uhler, J., Pink, M., and Herz, A. (2023). Insect Detect: An open-source DIY camera trap for automated insect monitoring. bioRxiv 2023, 12.05.570242. doi: 10.1101/2023.12.05.570242</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">First detection of Flavescence doreé phytoplasma in Phlogotettix cyclops (Hemiptera, Cicadellidae) and considerations on its possible role as vector in Austrian vineyards</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Reisenzein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IOBC-WPRS Bull</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Strauss, G., and Reisenzein, H. (2018). First detection of Flavescence doreé phytoplasma in Phlogotettix cyclops (Hemiptera, Cicadellidae) and considerations on its possible role as vector in Austrian vineyards. IOBC-WPRS Bull 139, 12-21.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Codling moth monitoring with camera-equipped automated traps: A review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Suto</surname></persName>
		</author>
		<idno type="DOI">10.3390/agriculture12101721</idno>
	</analytic>
	<monogr>
		<title level="j">Agriculture</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1721</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Suto, J. (2022). Codling moth monitoring with camera-equipped automated traps: A review. Agriculture 12, 1721. doi: 10.3390/agriculture12101721</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An expertized grapevine disease image database including five grape varieties focused on Flavescence doreé and its confounding diseases, biotic and abiotic stresses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tardif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keresztes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fontaine</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dib.2023.109230</idno>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">109230</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tardif, M., Amri, A., Deshayes, A., Greven, M., Keresztes, B., Fontaine, G., et al. (2023). An expertized grapevine disease image database including five grape varieties focused on Flavescence doreé and its confounding diseases, biotic and abiotic stresses. Data Brief 48, 109230. doi: 10.1016/j.dib.2023.109230</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Tardif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keresztes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tardif, M., Amri, A., Keresztes, B., Deshayes, A., Martin, D., Greven, M., et al. (2022).</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two-stage automatic diagnosis of Flavescence Doreé based on proximal imaging and artificial intelligence: a multi-year and multi-variety experimental study</title>
		<idno type="DOI">10.20870/oeno-one.2022.56.3.5460</idno>
	</analytic>
	<monogr>
		<title level="j">OENO One</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="371" to="384" />
		</imprint>
	</monogr>
	<note type="raw_reference">Two-stage automatic diagnosis of Flavescence Doreé based on proximal imaging and artificial intelligence: a multi-year and multi-variety experimental study. OENO One 56, 371-384. doi: 10.20870/oeno-one.2022.56.3.5460</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Detecting common coccinellids found in sorghum using deep learning models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Grijalva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccornack</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-023-36738-5</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">9748</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, C., Grijalva, I., Caragea, D., and McCornack, B. (2023). Detecting common coccinellids found in sorghum using deep learning models. Sci. Rep. 13, 9748. doi: 10.1038/s41598-023-36738-5</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Confidence score: the forgotten dimension of object detection performance evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alhazmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alrshoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21134350</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">4350</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wenkel, S., Alhazmi, K., Liiv, T., Alrshoud, S., and Simon, M. (2021). Confidence score: the forgotten dimension of object detection performance evaluation. Sensors 21, 4350. doi: 10.3390/s21134350</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019-04-15">2019. April 15, 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Girshick, R. (2019). Detectron2. Available online at: https://github.com/facebookresearch/detectron2 (Accessed April 15, 2024).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
